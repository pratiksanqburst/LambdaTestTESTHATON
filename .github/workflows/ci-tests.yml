name: CI - API divergence + generated tests (no inline Python)

on:
  workflow_dispatch: {}
  push:
    branches: [main]
  pull_request:
    branches: [main]

permissions:
  contents: read

jobs:
  generate-and-run:
    runs-on: ubuntu-latest
    env:
      SOURCE_DIR: backend
      OUTPUT_DIR: output

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # Ensure Python can import local `src` package
      - name: Add workspace to PYTHONPATH
        run: echo "PYTHONPATH=${GITHUB_WORKSPACE}" >> $GITHUB_ENV

      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Set up Node 20
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install Python requirements
        run: |
          python3 -m pip install --upgrade pip
          if [ -f requirements.txt ]; then python3 -m pip install -r requirements.txt; fi
          python3 -m pip install pyyaml openai || true
        shell: bash

      - name: Install Newman (local)
        run: |
          npm install newman newman-reporter-html
        shell: bash

      - name: Debug list discovered routes (script)
        # script should exist at scripts/debug_routes.py and import src.* modules
        run: |
          echo "Running scripts/debug_routes.py (SOURCE_DIR=${SOURCE_DIR})"
          if [ -f scripts/debug_routes.py ]; then
            python3 -u scripts/debug_routes.py || true
          else
            echo "scripts/debug_routes.py missing; skipping debug"
          fi
        shell: bash

      - name: Debug LLM secrets & gateway (shell-only, no python)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          QBURST_GATEWAY: ${{ secrets.QBURST_GATEWAY }}
        run: |
          echo "DEBUG: masked view of secrets & gateway (shell-only)"

          if [ -n "$OPENAI_API_KEY" ]; then
            prefix="${OPENAI_API_KEY:0:6}"
            len=$(printf '%s' "$OPENAI_API_KEY" | wc -c)
            echo "OPENAI_API_KEY present - prefix='${prefix}...' length=${len}"
          else
            echo "OPENAI_API_KEY not present"
          fi

          if [ -n "$QBURST_GATEWAY" ]; then
            gwlen=$(printf '%s' "$QBURST_GATEWAY" | wc -c)
            echo "QBURST_GATEWAY length=${gwlen}"
            echo "QBURST_GATEWAY hex (hexdump -C first 64 bytes):"
            printf '%s' "$QBURST_GATEWAY" | head -c 64 | hexdump -C -v || true
            echo "QBURST_GATEWAY displayed (raw):"
            printf '%s\n' "$QBURST_GATEWAY" || true
          else
            echo "QBURST_GATEWAY not present"
          fi

          # quick network check (HEAD request); will print response headers or an error
          if command -v curl >/dev/null 2>&1; then
            echo "curl -I to gateway (first 12 lines):"
            curl -sS -I --max-time 8 "$QBURST_GATEWAY" 2>&1 | sed -n '1,12p' || echo "curl failed (network or invalid URL)"
          else
            echo "curl not installed on runner"
          fi
        shell: bash

      # optionally start backend in-runner (uncomment if you need local server)
      # - name: Start backend (optional)
      #   run: |
      #     python3 -m pip install -r requirements.txt || true
      #     nohup python3 backend/sample_src_app.py > backend.log 2>&1 &
      #     for i in $(seq 1 12); do
      #       if curl -sS http://127.0.0.1:5000/ >/dev/null 2>&1; then
      #         echo "server up"
      #         break
      #       fi
      #       echo "waiting for server..."
      #       sleep 5
      #     done
      #   shell: bash

      - name: Run CI runner (generate collection & divergences)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          API_KEY: ${{ secrets.OPENAI_API_KEY }}
          QBURST_GATEWAY: ${{ secrets.QBURST_GATEWAY }}
        run: |
          mkdir -p "${OUTPUT_DIR}"
          # unbuffered for real-time logs
          python3 -u ci_runner.py --source-dir "${SOURCE_DIR}" --use-llm --gateway-url "${QBURST_GATEWAY}" --output-dir "${OUTPUT_DIR}"
        shell: bash

      - name: Show generated output files
        run: |
          echo "Contents of ${OUTPUT_DIR}:"
          ls -la "${OUTPUT_DIR}" || true
          if [ -f "${OUTPUT_DIR}/postman_collection.json" ]; then
            echo "postman_collection.json preview (first 120 lines):"
            head -n 120 "${OUTPUT_DIR}/postman_collection.json" || true
          else
            echo "postman_collection.json not found"
          fi
        shell: bash

      ####################################################################
      # REPLACED: Run newman on generated collection (we now push to HyperExecute)
      # This step triggers HyperExecute CLI, polls for result, downloads report.
      # Ensure the repo secrets LT_USERNAME and LT_ACCESS_KEY are set.
      ####################################################################
      - name: Run collection on HyperExecute (CLI) and fetch results
        env:
          LT_USERNAME: ${{ secrets.LT_USERNAME }}
          LT_ACCESS_KEY: ${{ secrets.LT_ACCESS_KEY }}
          HYPEREXECUTE_PROJECT: ${{ secrets.HYPEREXECUTE_PROJECT }}
          BASE_URL: ${{ secrets.BASE_URL }}
        run: |
          set -euo pipefail

          COLLECTION="${OUTPUT_DIR}/postman_collection.json"
          echo "==> HyperExecute run: check collection at $COLLECTION"

          if [ ! -f "$COLLECTION" ]; then
            echo "No collection found - skipping HyperExecute run"
            exit 0
          fi

          # ---- Install HyperExecute CLI (example) ----
          # Replace with actual package/install from your HyperExecute docs if different
          echo "Installing HyperExecute CLI (example package: @lambdatest/hyperexecute)..."
          npm install -g @lambdatest/hyperexecute || true

          # ---- Trigger HyperExecute run via CLI ----
          # Replace 'hyperexecute run' + flags with the actual CLI command & flags from your account docs if different
          echo "Triggering HyperExecute run..."
          hyperexecute run \
            --collection "$COLLECTION" \
            --env "{\"base_url\":\"${BASE_URL}\"}" \
            --project "${HYPEREXECUTE_PROJECT:-default}" \
            --username "${LT_USERNAME}" \
            --accessKey "${LT_ACCESS_KEY}" \
            --output hyperexecute-output.json \
            || true

          echo "=== CLI output summary ==="
          if [ -f hyperexecute-output.json ]; then
            jq -C . hyperexecute-output.json || cat hyperexecute-output.json || true
          else
            echo "hyperexecute-output.json not found; check CLI output above"
          fi

          # ---- Extract run id and dashboard url if present ----
          RUN_ID=$(jq -r '.runId // .id // ""' hyperexecute-output.json 2>/dev/null || echo "")
          DASHBOARD_URL=$(jq -r '.dashboardUrl // .reportUrl // ""' hyperexecute-output.json 2>/dev/null || echo "")

          echo "Run ID: ${RUN_ID:-<none>}"
          echo "Dashboard URL: ${DASHBOARD_URL:-<none>}"

          # ---- Poll for run completion (if a run id exists) ----
          if [ -n "$RUN_ID" ]; then
            echo "Polling HyperExecute status for runId=$RUN_ID..."
            MAX_TRIES=60       # ~20 minutes with SLEEP_SECONDS=20
            SLEEP_SECONDS=20
            for i in $(seq 1 $MAX_TRIES); do
              STATUS_JSON=$(curl -s -u "${LT_USERNAME}:${LT_ACCESS_KEY}" "https://hyperexecute.lambdatest.com/api/v1/runs/${RUN_ID}/status" || echo "")
              STATUS=$(echo "$STATUS_JSON" | jq -r '.status // ""' 2>/dev/null || echo "")
              echo "Attempt $i/$MAX_TRIES - status=${STATUS:-UNKNOWN}"
              if [ "$STATUS" = "COMPLETED" ] || [ "$STATUS" = "FINISHED" ] || [ "$STATUS" = "SUCCESS" ]; then
                echo "Run finished with status: $STATUS"
                break
              elif [ "$STATUS" = "FAILED" ] || [ "$STATUS" = "ERROR" ]; then
                echo "Run ended with failure status: $STATUS"
                break
              fi
              sleep $SLEEP_SECONDS
            done
          else
            echo "No RUN_ID available - skipping status poll"
          fi

          # ---- Try to download report (if supported by your API) ----
          if [ -n "$RUN_ID" ]; then
            echo "Attempting to download run report for runId=${RUN_ID}..."
            curl -s -u "${LT_USERNAME}:${LT_ACCESS_KEY}" "https://hyperexecute.lambdatest.com/api/v1/runs/${RUN_ID}/report" -o hyperexecute-report.zip || true
            if [ -f hyperexecute-report.zip ]; then
              echo "Downloaded hyperexecute-report.zip"
              ls -lh hyperexecute-report.zip || true
            else
              echo "No hyperexecute-report.zip found"
            fi
          fi

          # ---- Save metadata for upload ----
          echo "DASHBOARD_URL=${DASHBOARD_URL:-}" > hyperexecute-metadata.env || true
          echo "RUN_ID=${RUN_ID:-}" >> hyperexecute-metadata.env || true
          echo "HyperExecute step complete."

        shell: bash

      - name: Upload artifacts (HyperExecute + generated outputs)
        uses: actions/upload-artifact@v4
        with:
          name: api-test-artifacts
          path: |
            output/postman_collection.json
            output/divergence_report.json
            output/llm_error.log
            hyperexecute-output.json
            hyperexecute-metadata.env
            hyperexecute-report.zip

      - name: Comment PR with HyperExecute dashboard link
        if: ${{ github.event_name == 'pull_request' }}
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Extract dashboard link from hyperexecute-output.json or fallback to constructed URL
          DASHBOARD_URL=$(jq -r '.dashboardUrl // .reportUrl // ""' hyperexecute-output.json 2>/dev/null || echo "")
          RUN_ID=$(jq -r '.runId // .id // ""' hyperexecute-output.json 2>/dev/null || echo "")
          if [ -z "$DASHBOARD_URL" ] && [ -n "$RUN_ID" ]; then
            DASHBOARD_URL="https://hyperexecute.lambdatest.com/runs/${RUN_ID}"
          fi

          if [ -n "$DASHBOARD_URL" ]; then
            PR_NUMBER=${{ github.event.pull_request.number }}
            BODY=" HyperExecute run completed. Dashboard: ${DASHBOARD_URL}"
            echo "Posting comment to PR #${PR_NUMBER}"
            # use jq to safely produce JSON body
            jq -n --arg body "$BODY" '{body:$body}' | curl -s -X POST -H "Authorization: token ${GH_TOKEN}" -H "Content-Type: application/json" \
              "https://api.github.com/repos/${{ github.repository }}/issues/${PR_NUMBER}/comments" -d @-
          else
            echo "No dashboard URL present; skipping PR comment"
          fi
        shell: bash
